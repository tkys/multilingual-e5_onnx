{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/XmlOqV4Z2gL8VhjhREdo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tkys/multilingual-e5_onnx/blob/main/multilingual_e5_onnxruntime.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNBHeqfiGL82",
        "outputId": "65aaf519-1b27-4c1e-fdbc-dbf7cd0bb2fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'multilingual-e5-small' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://huggingface.co/intfloat/multilingual-e5-small"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -alh ./multilingual-e5-small/onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuQ6mF03GSmi",
        "outputId": "bd439e58-e9fe-49c0-9629-60f4f153c887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 470M\n",
            "drwxr-xr-x 2 root root 4.0K Feb  5 23:49 .\n",
            "drwxr-xr-x 5 root root 4.0K Feb  5 23:49 ..\n",
            "-rw-r--r-- 1 root root  653 Feb  5 23:49 config.json\n",
            "-rw-r--r-- 1 root root 449M Feb  5 23:49 model.onnx\n",
            "-rw-r--r-- 1 root root 4.9M Feb  5 23:49 sentencepiece.bpe.model\n",
            "-rw-r--r-- 1 root root  167 Feb  5 23:49 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root  443 Feb  5 23:49 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  17M Feb  5 23:49 tokenizer.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install onnx onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpMOPLgkGk5U",
        "outputId": "7ac02ea9-751e-40af-a776-3c2dad31d78c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.15.0)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.12)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##　onnxモデルがどんなIN/OUTか確認するだけ\n",
        "\n",
        "import onnx\n",
        "\n",
        "# ONNXモデルのロード\n",
        "model_path = \"/content/multilingual-e5-small/onnx/model.onnx\"\n",
        "model = onnx.load(model_path)\n",
        "\n",
        "\n",
        "# モデルの入力と出力の情報\n",
        "\n",
        "print(\"\\n## モデルの入力:\")\n",
        "for input in model.graph.input:\n",
        "    print(f\"{input.name}, {input.type}\")\n",
        "\n",
        "print(\"\\n##モデルの出力:\")\n",
        "for output in model.graph.output:\n",
        "    print(f\"{output.name}, {output.type}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea4SM7sNorCN",
        "outputId": "453a64c5-833c-4ab0-c45c-e2c3dcab1c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## モデルの入力:\n",
            "input_ids, tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_param: \"batch_size\"\n",
            "    }\n",
            "    dim {\n",
            "      dim_param: \"sequence_length\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "attention_mask, tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_param: \"batch_size\"\n",
            "    }\n",
            "    dim {\n",
            "      dim_param: \"sequence_length\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "token_type_ids, tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_param: \"batch_size\"\n",
            "    }\n",
            "    dim {\n",
            "      dim_param: \"sequence_length\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "##モデルの出力:\n",
            "last_hidden_state, tensor_type {\n",
            "  elem_type: 1\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_param: \"batch_size\"\n",
            "    }\n",
            "    dim {\n",
            "      dim_param: \"sequence_length\"\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 384\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "import onnxruntime as ort\n",
        "from transformers import AutoTokenizer\n"
      ],
      "metadata": {
        "id": "NvpFijmGHwwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "0N4bcrmZNGDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. トークナイザー\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/multilingual-e5-small/onnx\") #dirパスごと指定\n",
        "\n",
        "# 2. ONNXモデルパス\n",
        "onnx_model_path = \"/content/multilingual-e5-small/onnx/model.onnx\"\n",
        "\n",
        "# 3. runtime session 作成\n",
        "session = ort.InferenceSession(onnx_model_path)\n"
      ],
      "metadata": {
        "id": "X6Zgzdc1MI7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# onnxモデルの入力ノードdictの確認 # 👆で既に確認したが\n",
        "input_names = [input.name for input in session.get_inputs()]\n",
        "\n",
        "print(\" onnxモデルの入力ノード:\", input_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hf1se6s2jZze",
        "outputId": "6d6ad273-bdc1-4800-eb06-ed6385872864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " onnxモデルの入力ノード: ['input_ids', 'attention_mask', 'token_type_ids']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 入力テキスト\n",
        "text = \"ディスプレイがつかない。 \"\n",
        "\n",
        "# tokenize\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n"
      ],
      "metadata": {
        "id": "vj_3bqJLPGM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#見てみる\n",
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOPOH9FOsS2_",
        "outputId": "ba6b835f-79e1-4948-d7eb-75afb4bc1360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[     0,      6,  39601,   3385, 125954,    281, 226591,     30,      6,\n",
              "              2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  onnxモデルへの入力ノードに沿ったデータ作成する\n",
        "input_feed = {\n",
        "    'input_ids': inputs['input_ids'].numpy(),\n",
        "    'attention_mask': inputs['attention_mask'].numpy(),\n",
        "}\n",
        "\n",
        "# input_feed[`token_type_ids`]も必要だがもって無いので、手動で追加する（全て0の配列を生成）\n",
        "input_feed['token_type_ids'] = np.zeros_like(inputs['input_ids'].numpy())\n",
        "\n"
      ],
      "metadata": {
        "id": "eYFlSEwFsVKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#見てみる\n",
        "input_feed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEiGlAqAhmAt",
        "outputId": "6d3f9222-9a8c-4422-f52e-b6bb18f6cf97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': array([[     0,      6,  39601,   3385, 125954,    281, 226591,     30,\n",
              "              6,      2]]),\n",
              " 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
              " 'token_type_ids': array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# モデルに入力\n",
        "outputs = session.run(None, input_feed) # session.run(output_name,input_feed)　 output_nameはNoneでもいいが、👆で確認したモデルの出力ノード名[\"last_hidden_state\"]を明示指定してもOK\n",
        "\n",
        "# 出力ベクトル取得\n",
        "encoded_vector = outputs[0]\n",
        "\n",
        "#\n",
        "print(encoded_vector)\n",
        "print(encoded_vector.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wWq4bcUs1V-",
        "outputId": "5d26c9d5-4101-4f0a-8169-376adf9b657c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 0.21865204 -0.04455148 -0.18693127 ...  0.10748932  0.08861979\n",
            "    0.04202938]\n",
            "  [ 0.23228452 -0.0442718  -0.0215985  ...  0.18467855  0.30522713\n",
            "    0.2824254 ]\n",
            "  [ 0.21584222 -0.07112919 -0.1089538  ...  0.01026974  0.04115607\n",
            "    0.15442488]\n",
            "  ...\n",
            "  [ 0.25109327 -0.03354806 -0.152942   ...  0.24721311  0.31745362\n",
            "    0.29802948]\n",
            "  [ 0.2451328  -0.00767547 -0.01601937 ...  0.15090165  0.26564133\n",
            "    0.25788653]\n",
            "  [ 0.2186499  -0.04454774 -0.18693393 ...  0.1074903   0.08862248\n",
            "    0.04202624]]]\n",
            "(1, 10, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session.run(None, input_feed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pkbiEVDBFZY",
        "outputId": "6f1a70bc-b508-4345-80af-a89fa2f1c2de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[[ 0.21865204, -0.04455148, -0.18693127, ...,  0.10748932,\n",
              "           0.08861979,  0.04202938],\n",
              "         [ 0.23228452, -0.0442718 , -0.0215985 , ...,  0.18467855,\n",
              "           0.30522713,  0.2824254 ],\n",
              "         [ 0.21584222, -0.07112919, -0.1089538 , ...,  0.01026974,\n",
              "           0.04115607,  0.15442488],\n",
              "         ...,\n",
              "         [ 0.25109327, -0.03354806, -0.152942  , ...,  0.24721311,\n",
              "           0.31745362,  0.29802948],\n",
              "         [ 0.2451328 , -0.00767547, -0.01601937, ...,  0.15090165,\n",
              "           0.26564133,  0.25788653],\n",
              "         [ 0.2186499 , -0.04454774, -0.18693393, ...,  0.1074903 ,\n",
              "           0.08862248,  0.04202624]]], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#関数化\n",
        "def emcode_onnx(session,input_text:str):\n",
        "  inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "  # 入力辞書の準備\n",
        "  input_feed = {\n",
        "      'input_ids': inputs['input_ids'].numpy(),\n",
        "      'attention_mask': inputs['attention_mask'].numpy(),\n",
        "  }\n",
        "\n",
        "  # input_feed[`token_type_ids`]が必要なので、手動で追加（全て0の配列を生成）\n",
        "  input_feed['token_type_ids'] = np.zeros_like(inputs['input_ids'].numpy())\n",
        "\n",
        "\n",
        "  # モデルへ入力\n",
        "  outputs = session.run(None, input_feed)\n",
        "\n",
        "  # 出力ベクトル取得\n",
        "  encoded_vector = outputs[0]\n",
        "\n",
        "  #print(encoded_vector)\n",
        "\n",
        "  return encoded_vector\n",
        "\n"
      ],
      "metadata": {
        "id": "O4eglAmCPMbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Start using open-source software in minutes\"\n",
        "\n",
        "print(emcode_onnx(session,text))\n",
        "print(emcode_onnx(session,text).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHM1nz0ynWpG",
        "outputId": "98145929-86e5-4e1b-d3e2-2b3427cac519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 0.17999783 -0.04057322 -0.19378749 ...  0.23418048  0.03804934\n",
            "    0.05502447]\n",
            "  [ 0.18267345 -0.1542196   0.21307006 ...  0.48656797  0.16380218\n",
            "    0.2404509 ]\n",
            "  [ 0.00050503 -0.27400565  0.04302146 ...  0.3787657  -0.13050175\n",
            "    0.21006621]\n",
            "  ...\n",
            "  [ 0.07364069 -0.04141909 -0.02288921 ...  0.43417826 -0.12035015\n",
            "    0.20888083]\n",
            "  [-0.08164199  0.02301908 -0.23771703 ...  0.35335115 -0.02888722\n",
            "    0.17335638]\n",
            "  [ 0.17999843 -0.04056928 -0.1937894  ...  0.23418029  0.03805151\n",
            "    0.05502164]]]\n",
            "(1, 10, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#テキスト類似度関数\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# コサイン類似度\n",
        "def cos_sim(encoded_vector1,encoded_vector2):\n",
        "\n",
        "  # テキストtokenベクトルの平均を計算\n",
        "  mean_vector1 = np.mean(encoded_vector1, axis=1)\n",
        "  mean_vector2 = np.mean(encoded_vector2, axis=1)\n",
        "\n",
        "  # コサイン類似度\n",
        "  similarity = cosine_similarity(mean_vector1, mean_vector2)\n",
        "\n",
        "  return similarity\n"
      ],
      "metadata": {
        "id": "mpmFvfDjb9Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "Dd89abshatJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_text = \"能登半島地震\"\n",
        "\n",
        "news_list = \"\"\"\n",
        "日本のアジア杯敗退に“隠せぬ本音”を吐露 ブライトン指揮官が三笘薫の重要性を語る「日本人を愛しているが、私は嬉しい」（CoCoKARAnext）昨日JBpress\n",
        "能動的サイバー防御を詳解：日本はいつまでサイバー攻撃に丸腰でいるのか 日本が対応できない法的問題、技術的問題と各国の情勢(1/12)4 日経ビジネスオンライン\n",
        "曲がる次世代太陽電池 日本発の有望技術に中国の足音再び5\n",
        "ビジネス米テスラ株が下落、独ＳＡＰの車両調達打ち切り報道などで1\n",
        "円は対ドルで148円台後半に下落、ＩＳＭ指数を受けた米金利一段高で3 日経平均株価\n",
        "歌手のテイラー・スウィフトさん 米グラミー賞で史上最多、4度目の最優秀アルバム賞に(2024年2月5日)14\n",
        "チャールズ英国王「がん」治療開始、英王室は詳しい病状明らかにせず…入院もせず事務的執務を継続6\n",
        "ＮＨＫ朝ドラ「ブギウギ」 スズ子役の趣里、トミ役の小雪と念願の〝仲良し写真〟…視聴者「愛助も見たかったでしょうね」22 物理学東京大学\n",
        "音波を閉じ込めてスピン波との強結合を室温で実証 －スピン波-音波を活用した新しいデバイスへ道－ | 物性研究所5\n",
        "日前重力っていったい何？ すべての物に力が働く、空間のひずみ：朝日新聞デジタル3 日前日本経済新聞\n",
        "高エネルギー加速器研究機構、国内最大の加速器再始動 「反物質」の謎を探索4 日前\n",
        "発電側課金導入で４月から規制料金変更／北海道・北陸・四国の３社が申請4\n",
        "東電管内で１．３万軒停電＝東京など６都県、原因調査中10\n",
        "石川 能登町の水道復旧支援 長浜水道企業団の職員らが出発｜NHK 滋賀県のニュース22 インターネット セキュリティ外務省公電情報が漏洩 中国からサイバー攻撃 閉域システムに侵入21\n",
        "中国のネット監視・検閲「グレートファイアウォール」を個人で再現 オープンソース「OpenGFW」公開中2 Yahoo!ニュース\n",
        "中国支援のサイバー攻撃用ネットワーク解体、米当局発表（ＡＦＰ＝時事）5 日前ニュース提供元\n",
        "ハマス戦闘員半数以上死傷か\n",
        "「完全勝利」まで戦闘継続＝イスラエル首相国王の決定で公表\n",
        "英国王、がんと診断　王室は種類明かさず2日の空爆前に\n",
        "米、イラクに事前通告せず ＝国務省\n",
        "ブタの腎臓を胎児に移植、透析までの「橋渡し」に…国内初「異種移植」を慈恵医大など計画音楽\n",
        "テイラー・スウィフトさん「人生最高の瞬間」、史上最多４回目のグラミー最優秀アルバム賞国際\n",
        "ゼレンスキー氏、ザルジニー総司令官の解任検討初めて認める…「リセットが必要だ」\n",
        "放置された築200年の古民家は多くの支えで再生した　カフェ「かやぶき日記」新たな日々愛知\n",
        "エスカレーター歩くと「条例違反です」ＡＩ声掛け　名古屋市営地下鉄で実証実験岐阜\n",
        "「柿畑は財産だ」富有柿の定番手土産を開発中　瑞穂の農家の第１弾「最中アイス」\n",
        "プーチン　ウクライナ　戦争　ロシア軍　降伏\n",
        "「主導権は完全に露軍に移った」プーチン氏、ウクライナに降伏要求中国　サッカー　腐敗　代表監督　弱い\n",
        "中国サッカーが弱い理由は…代表監督まで腐敗まみれ前橋市長選　自民現職　敗北　前橋ショック　小川晶\n",
        "王国で自民現職敗れる〝前橋ショック〟「政治不信が…」「政権交代起きてもおかしくない」\n",
        "\"\"\"\n",
        "\n",
        "target_list = news_list.split(\"\\n\")\n",
        "\n",
        "encode_time=[]\n",
        "encode_token_len=[]\n",
        "for target in target_list:\n",
        "\n",
        "  encode_vec = emcode_onnx(session,target)\n",
        "\n",
        "  st=time.time()\n",
        "  query_vec  = emcode_onnx(session,query_text)\n",
        "  encode_time.append(time.time()-st)\n",
        "\n",
        "  encode_token_len.append(encode_vec.shape[1])\n",
        "\n",
        "  print(f\"cos_sim {format(cos_sim(encode_vec,query_vec)[0][0],'.3f')}\\t{query_text} : {target}\")\n",
        "\n",
        "\n",
        "print(f\"\\n\")\n",
        "print(f\"encode_time_sum:{sum(encode_time)}\")\n",
        "print(f\"encode_token_len_sum:{sum(encode_token_len)}\")\n",
        "print(f\"encode speed (token/sec):{sum(encode_token_len)/sum(encode_time)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLMlJSx_PnlG",
        "outputId": "765114c9-efc1-4493-e89d-153f67c0af5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cos_sim 0.800\t能登半島地震 : \n",
            "cos_sim 0.787\t能登半島地震 : 日本のアジア杯敗退に“隠せぬ本音”を吐露 ブライトン指揮官が三笘薫の重要性を語る「日本人を愛しているが、私は嬉しい」（CoCoKARAnext）昨日JBpress\n",
            "cos_sim 0.835\t能登半島地震 : 能動的サイバー防御を詳解：日本はいつまでサイバー攻撃に丸腰でいるのか 日本が対応できない法的問題、技術的問題と各国の情勢(1/12)4 日経ビジネスオンライン\n",
            "cos_sim 0.812\t能登半島地震 : 曲がる次世代太陽電池 日本発の有望技術に中国の足音再び5 \n",
            "cos_sim 0.811\t能登半島地震 : ビジネス米テスラ株が下落、独ＳＡＰの車両調達打ち切り報道などで1 \n",
            "cos_sim 0.786\t能登半島地震 : 円は対ドルで148円台後半に下落、ＩＳＭ指数を受けた米金利一段高で3 日経平均株価\n",
            "cos_sim 0.777\t能登半島地震 : 歌手のテイラー・スウィフトさん 米グラミー賞で史上最多、4度目の最優秀アルバム賞に(2024年2月5日)14 \n",
            "cos_sim 0.805\t能登半島地震 : チャールズ英国王「がん」治療開始、英王室は詳しい病状明らかにせず…入院もせず事務的執務を継続6 \n",
            "cos_sim 0.820\t能登半島地震 : ＮＨＫ朝ドラ「ブギウギ」 スズ子役の趣里、トミ役の小雪と念願の〝仲良し写真〟…視聴者「愛助も見たかったでしょうね」22 物理学東京大学\n",
            "cos_sim 0.789\t能登半島地震 : 音波を閉じ込めてスピン波との強結合を室温で実証 －スピン波-音波を活用した新しいデバイスへ道－ | 物性研究所5 \n",
            "cos_sim 0.824\t能登半島地震 : 日前重力っていったい何？ すべての物に力が働く、空間のひずみ：朝日新聞デジタル3 日前日本経済新聞\n",
            "cos_sim 0.818\t能登半島地震 : 高エネルギー加速器研究機構、国内最大の加速器再始動 「反物質」の謎を探索4 日前\n",
            "cos_sim 0.804\t能登半島地震 : 発電側課金導入で４月から規制料金変更／北海道・北陸・四国の３社が申請4 \n",
            "cos_sim 0.845\t能登半島地震 : 東電管内で１．３万軒停電＝東京など６都県、原因調査中10 \n",
            "cos_sim 0.856\t能登半島地震 : 石川 能登町の水道復旧支援 長浜水道企業団の職員らが出発｜NHK 滋賀県のニュース22 インターネット セキュリティ外務省公電情報が漏洩 中国からサイバー攻撃 閉域システムに侵入21 \n",
            "cos_sim 0.793\t能登半島地震 : 中国のネット監視・検閲「グレートファイアウォール」を個人で再現 オープンソース「OpenGFW」公開中2 Yahoo!ニュース\n",
            "cos_sim 0.815\t能登半島地震 : 中国支援のサイバー攻撃用ネットワーク解体、米当局発表（ＡＦＰ＝時事）5 日前ニュース提供元\n",
            "cos_sim 0.828\t能登半島地震 : ハマス戦闘員半数以上死傷か\n",
            "cos_sim 0.803\t能登半島地震 : 「完全勝利」まで戦闘継続＝イスラエル首相国王の決定で公表\n",
            "cos_sim 0.814\t能登半島地震 : 英国王、がんと診断　王室は種類明かさず2日の空爆前に\n",
            "cos_sim 0.822\t能登半島地震 : 米、イラクに事前通告せず ＝国務省\n",
            "cos_sim 0.791\t能登半島地震 : ブタの腎臓を胎児に移植、透析までの「橋渡し」に…国内初「異種移植」を慈恵医大など計画音楽\n",
            "cos_sim 0.775\t能登半島地震 : テイラー・スウィフトさん「人生最高の瞬間」、史上最多４回目のグラミー最優秀アルバム賞国際\n",
            "cos_sim 0.798\t能登半島地震 : ゼレンスキー氏、ザルジニー総司令官の解任検討初めて認める…「リセットが必要だ」\n",
            "cos_sim 0.802\t能登半島地震 : 放置された築200年の古民家は多くの支えで再生した　カフェ「かやぶき日記」新たな日々愛知\n",
            "cos_sim 0.791\t能登半島地震 : エスカレーター歩くと「条例違反です」ＡＩ声掛け　名古屋市営地下鉄で実証実験岐阜\n",
            "cos_sim 0.790\t能登半島地震 : 「柿畑は財産だ」富有柿の定番手土産を開発中　瑞穂の農家の第１弾「最中アイス」\n",
            "cos_sim 0.837\t能登半島地震 : プーチン　ウクライナ　戦争　ロシア軍　降伏　\n",
            "cos_sim 0.837\t能登半島地震 : 「主導権は完全に露軍に移った」プーチン氏、ウクライナに降伏要求中国　サッカー　腐敗　代表監督　弱い　\n",
            "cos_sim 0.819\t能登半島地震 : 中国サッカーが弱い理由は…代表監督まで腐敗まみれ前橋市長選　自民現職　敗北　前橋ショック　小川晶\n",
            "cos_sim 0.824\t能登半島地震 : 王国で自民現職敗れる〝前橋ショック〟「政治不信が…」「政権交代起きてもおかしくない」\n",
            "cos_sim 0.800\t能登半島地震 : \n",
            "\n",
            "\n",
            "encode_time_sum:0.6443085670471191\n",
            "encode_token_len_sum:1001\n",
            "encode speed (token/sec):1553.6034303992042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_vec.shape[1]"
      ],
      "metadata": {
        "id": "aDzPXPnv9UcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJcTHhW89UV-",
        "outputId": "f050d215-0529-40d5-dd4a-64e1b69fde11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##比較\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_name = \"intfloat/multilingual-e5-small\"\n",
        "\n",
        "# モデルのロード\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "\n",
        "st = time.time()\n",
        "# 例えば、以下のクエリベクトルを使用して検索を実行する\n",
        "query_vector = model.encode(target_list)\n",
        "\n",
        "encode_time_sum = time.time()-st\n",
        "\n",
        "print(f\"\\n## Benchmark ##\\n\")\n",
        "print(f\"encode_time.sum:{encode_time_sum}\")\n",
        "print(f\"encode_token_len_sum:{sum(encode_token_len)}\")\n",
        "print(f\"encode speed (token/sec):{sum(encode_token_len)/encode_time_sum}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fE7QaXSx8AIA",
        "outputId": "658a3c0e-e31c-4f2b-b838-903fae35736a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## Benchmark ##\n",
            "\n",
            "encode_time.sum:5.049156904220581\n",
            "encode_token_len_sum:1001\n",
            "encode speed (token/sec):198.25091970567718\n"
          ]
        }
      ]
    }
  ]
}